{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python390jvsc74a57bd063fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d",
   "display_name": "Python 3.9.0 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Model for HackMX NDS Cognitive Labs Challenge\n",
    "## Fraud Detection\n",
    "#### Dataset obtained from IEEE-CIS Fraud Detection in Kaggle: "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import TimeSeriesSplit, KFold\n",
    "\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_identity = \"data/train_identity.csv\"\n",
    "train_transaction = \"data/train_transaction.csv\"\n",
    "test_identity = \"data/test_identity.csv\"\n",
    "test_transaction = \"data/test_transaction.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time # visualize time to load data\n",
    "train_id = pd.read_csv(train_identity)\n",
    "train_tr = pd.read_csv(train_transaction)\n",
    "test_id = pd.read_csv(test_identity)\n",
    "test_tr = pd.read_csv(test_transaction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downcast_dtypes(df):\n",
    "    _start = df.memory_usage(deep=True).sum() / 1024 ** 2\n",
    "    float_cols = [c for c in df if df[c].dtype == \"float64\"]\n",
    "    int_cols = [c for c in df if df[c].dtype in [\"int64\", \"int32\"]]\n",
    "    df[float_cols] = df[float_cols].astype(np.float32)\n",
    "    df[int_cols] = df[int_cols].astype(np.int16)\n",
    "    _end = df.memory_usage(deep=True).sum() / 1024 ** 2\n",
    "    saved = (_start - _end) / _start * 100\n",
    "    print(f\"Saved {saved:.2f}%\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_id = downcast_dtypes(train_id)\n",
    "train_tr = downcast_dtypes(train_tr)\n",
    "test_id = downcast_dtypes(test_id)\n",
    "test_tr = downcast_dtypes(test_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.merge(\n",
    "    train_tr, train_id, how=\"left\", on=\"TransactionID\", left_index=True, right_index=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv(\"train_set.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.merge(\n",
    "    test_tr, test_id, how=\"left\", on=\"TransactionID\", left_index=True, right_index=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Train shape: {train.shape}, Test shape: {test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_features = [\n",
    "    \"TransactionAmt\",\n",
    "    \"ProductCD\",\n",
    "    \"card1\",\n",
    "    \"card2\",\n",
    "    \"card3\",\n",
    "    \"card5\",\n",
    "    \"card6\",\n",
    "    \"addr1\",\n",
    "    \"addr2\",\n",
    "    \"dist1\",\n",
    "    \"dist2\",\n",
    "    \"P_emaildomain\",\n",
    "    \"R_emaildomain\",\n",
    "    \"C1\",\n",
    "    \"C2\",\n",
    "    \"C4\",\n",
    "    \"C5\",\n",
    "    \"C6\",\n",
    "    \"C7\",\n",
    "    \"C8\",\n",
    "    \"C9\",\n",
    "    \"C10\",\n",
    "    \"C11\",\n",
    "    \"C12\",\n",
    "    \"C13\",\n",
    "    \"C14\",\n",
    "    \"D1\",\n",
    "    \"D2\",\n",
    "    \"D3\",\n",
    "    \"D4\",\n",
    "    \"D5\",\n",
    "    \"D10\",\n",
    "    \"D11\",\n",
    "    \"D15\",\n",
    "    \"M1\",\n",
    "    \"M2\",\n",
    "    \"M3\",\n",
    "    \"M4\",\n",
    "    \"M6\",\n",
    "    \"M7\",\n",
    "    \"M8\",\n",
    "    \"M9\",\n",
    "    \"V1\",\n",
    "    \"V3\",\n",
    "    \"V4\",\n",
    "    \"V6\",\n",
    "    \"V8\",\n",
    "    \"V11\",\n",
    "    \"V13\",\n",
    "    \"V14\",\n",
    "    \"V17\",\n",
    "    \"V20\",\n",
    "    \"V23\",\n",
    "    \"V26\",\n",
    "    \"V27\",\n",
    "    \"V30\",\n",
    "    \"V36\",\n",
    "    \"V37\",\n",
    "    \"V40\",\n",
    "    \"V41\",\n",
    "    \"V44\",\n",
    "    \"V47\",\n",
    "    \"V48\",\n",
    "    \"V54\",\n",
    "    \"V56\",\n",
    "    \"V59\",\n",
    "    \"V62\",\n",
    "    \"V65\",\n",
    "    \"V67\",\n",
    "    \"V68\",\n",
    "    \"V70\",\n",
    "    \"V76\",\n",
    "    \"V78\",\n",
    "    \"V80\",\n",
    "    \"V82\",\n",
    "    \"V86\",\n",
    "    \"V88\",\n",
    "    \"V89\",\n",
    "    \"V91\",\n",
    "    \"V107\",\n",
    "    \"V108\",\n",
    "    \"V111\",\n",
    "    \"V115\",\n",
    "    \"V117\",\n",
    "    \"V120\",\n",
    "    \"V121\",\n",
    "    \"V123\",\n",
    "    \"V124\",\n",
    "    \"V127\",\n",
    "    \"V129\",\n",
    "    \"V130\",\n",
    "    \"V136\",\n",
    "    \"V138\",\n",
    "    \"V139\",\n",
    "    \"V142\",\n",
    "    \"V147\",\n",
    "    \"V156\",\n",
    "    \"V160\",\n",
    "    \"V162\",\n",
    "    \"V165\",\n",
    "    \"V166\",\n",
    "    \"V169\",\n",
    "    \"V171\",\n",
    "    \"V173\",\n",
    "    \"V175\",\n",
    "    \"V176\",\n",
    "    \"V178\",\n",
    "    \"V180\",\n",
    "    \"V182\",\n",
    "    \"V185\",\n",
    "    \"V187\",\n",
    "    \"V188\",\n",
    "    \"V198\",\n",
    "    \"V203\",\n",
    "    \"V205\",\n",
    "    \"V207\",\n",
    "    \"V209\",\n",
    "    \"V210\",\n",
    "    \"V215\",\n",
    "    \"V218\",\n",
    "    \"V220\",\n",
    "    \"V221\",\n",
    "    \"V223\",\n",
    "    \"V224\",\n",
    "    \"V226\",\n",
    "    \"V228\",\n",
    "    \"V229\",\n",
    "    \"V234\",\n",
    "    \"V235\",\n",
    "    \"V238\",\n",
    "    \"V240\",\n",
    "    \"V250\",\n",
    "    \"V252\",\n",
    "    \"V253\",\n",
    "    \"V257\",\n",
    "    \"V258\",\n",
    "    \"V260\",\n",
    "    \"V261\",\n",
    "    \"V264\",\n",
    "    \"V266\",\n",
    "    \"V267\",\n",
    "    \"V271\",\n",
    "    \"V274\",\n",
    "    \"V277\",\n",
    "    \"V281\",\n",
    "    \"V283\",\n",
    "    \"V284\",\n",
    "    \"V285\",\n",
    "    \"V286\",\n",
    "    \"V289\",\n",
    "    \"V291\",\n",
    "    \"V294\",\n",
    "    \"V296\",\n",
    "    \"V297\",\n",
    "    \"V301\",\n",
    "    \"V303\",\n",
    "    \"V305\",\n",
    "    \"V307\",\n",
    "    \"V309\",\n",
    "    \"V310\",\n",
    "    \"V314\",\n",
    "    \"V320\",\n",
    "    \"DeviceType\",\n",
    "    \"DeviceInfo\",\n",
    "    \"isFraud\", ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop_train = [col for col in train.columns if col not in imp_features]\n",
    "cols_to_drop_test = [col for col in test.columns if col not in imp_features]\n",
    "\n",
    "train = train.drop(cols_to_drop_train, axis=1)\n",
    "test = test.drop(cols_to_drop_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.replace([np.inf, -np.inf], np.nan)\n",
    "test = test.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "train.fillna(0, inplace=True)\n",
    "test.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in train.columns:\n",
    "    if train[col].dtype == \"object\":\n",
    "        le = LabelEncoder()\n",
    "        le.fit(list(train[col].astype(str).values) + list(test[col].astype(str).values))\n",
    "        train[col] = le.transform(list(train[col].astype(str).values))\n",
    "        test[col] = le.transform(list(test[col].astype(str).values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train.shape)\n",
    "print(test.shape)\n",
    "train.to_csv(\"train_set.csv\", index = False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train.drop(\"isFraud\", axis=1).copy()\n",
    "X_test = test.copy()\n",
    "y_train = train[\"isFraud\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape, X_test.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_split, X_test_split, y_train_split, y_test_split = train_test_split(\n",
    "    X_train, y_train, test_size=0.3, random_state=7\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(\n",
    "    max_depth=45, max_features=30, n_estimators=500, n_jobs=-1, min_samples_leaf=200\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time \n",
    "rf.fit(X_train_split, y_train_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Roc Auc Score:\", roc_auc_score(y_test_split, rf.predict(X_test_split)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pkl_Filename = \"modelo.pkl\"  \n",
    "\n",
    "with open(Pkl_Filename, 'wb') as file:  \n",
    "    pickle.dump(rf, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = {}\n",
    "for feature, importance in zip(X_train.columns, rf.feature_importances_):\n",
    "    feats[feature] = importance\n",
    "\n",
    "importances = pd.DataFrame.from_dict(feats, orient=\"index\").rename(\n",
    "    columns={0: \"Gini-importance\"}\n",
    ")\n",
    "imp = importances.sort_values(by=\"Gini-importance\", ascending=False)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1, figsize=(16, 7))\n",
    "plt.bar(imp.index, imp[\"Gini-importance\"])\n",
    "plt.xticks(imp.index, rotation=90)\n",
    "print(imp.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit, KFold\n",
    "\n",
    "n_folds = 5\n",
    "folds = TimeSeriesSplit(n_splits=n_folds)\n",
    "folds = KFold(n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = X_train.columns\n",
    "\n",
    "splits = folds.split(X_train, y_train)\n",
    "\n",
    "y_preds = np.zeros(X_test.shape[0])\n",
    "y_oof = np.zeros(X_train.shape[0])\n",
    "\n",
    "score_auc = 0\n",
    "\n",
    "feature_importances = pd.DataFrame()\n",
    "feature_importances[\"feature\"] = columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"num_leaves\": 64,\n",
    "    \"min_child_weight\": 0.03,\n",
    "    \"feature_fraction\": 0.04,\n",
    "    \"bagging_fraction\": 0.33,\n",
    "    \"min_data_in_leaf\": 80,\n",
    "    \"objective\": \"binary\",\n",
    "    \"max_depth\": -1,\n",
    "    \"learning_rate\": 0.006,\n",
    "    \"boosting_type\": \"gbdt\",\n",
    "    \"bagging_seed\": 7,\n",
    "    \"metric\": \"auc\",\n",
    "    \"verbosity\": -1,\n",
    "    \"reg_alpha\": 0.3,\n",
    "    \"reg_lambda\": 0.6,\n",
    "    \"random_state\": 0,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fold_n, (train_index, valid_index) in enumerate(splits):\n",
    "    X_tr, X_val = X_train[columns].iloc[train_index], X_train[columns].iloc[valid_index]\n",
    "    y_tr, y_val = y_train.iloc[train_index], y_train.iloc[valid_index]\n",
    "    \n",
    "    dtrain = lgb.Dataset(X_tr, label=y_tr)\n",
    "    dvalid = lgb.Dataset(X_val, label=y_val)\n",
    "\n",
    "    clf = lgb.train(params, dtrain, 10000, valid_sets = [dtrain, dvalid], verbose_eval=200, early_stopping_rounds=100)\n",
    "    \n",
    "    feature_importances[f'fold_{fold_n + 1}'] = clf.feature_importance()\n",
    "    \n",
    "    y_pred_val = clf.predict(X_val)\n",
    "    y_oof[valid_index] = y_pred_val\n",
    "    print(f\"Fold {fold_n + 1} | AUC: {roc_auc_score(y_val, y_pred_val)}\")\n",
    "    \n",
    "    score_auc += roc_auc_score(y_val, y_pred_val) / n_folds\n",
    "    \n",
    "    y_preds += clf.predict(X_test) / n_folds\n",
    "    \n",
    "    del X_tr, X_val, y_tr, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nMean AUC = {score_auc}\")\n",
    "print(f\"Out of folds AUC = {roc_auc_score(y_train, y_oof)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances[\"average\"] = feature_importances[\n",
    "    [\"fold_{}\".format(fold + 1) for fold in range(folds.n_splits)]\n",
    "].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = (\n",
    "    feature_importances[[\"feature\", \"average\"]]\n",
    "    .sort_values(by=\"average\", ascending=False)\n",
    "    .head(20)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1, figsize=(16, 7))\n",
    "plt.bar(f[\"feature\"], f[\"average\"])\n",
    "plt.xticks(f[\"feature\"], rotation=90)\n",
    "print(imp.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_params = {'boosting_type': 'gbdt',\n",
    " 'class_weight': None,\n",
    " 'colsample_bytree': 1.0,\n",
    " 'importance_type': 'split',\n",
    " 'learning_rate': 0.006,\n",
    " 'max_depth': -1,\n",
    " 'min_child_samples': 20,\n",
    " 'min_child_weight': 0.03,\n",
    " 'min_split_gain': 0.0,\n",
    " 'n_estimators': 100,\n",
    " 'n_jobs': -1,\n",
    " 'num_leaves': 64,\n",
    " 'objective': 'binary',\n",
    " 'random_state': 0,\n",
    " 'reg_alpha': 0.3,\n",
    " 'reg_lambda': 0.6,\n",
    " 'silent': True,\n",
    " 'subsample': 1.0,\n",
    " 'subsample_for_bin': 200000,\n",
    " 'subsample_freq': 0,\n",
    " 'feature_fraction': 0.04,\n",
    " 'bagging_fraction': 0.33,\n",
    " 'min_data_in_leaf': 80,\n",
    " 'bagging_seed': 7,\n",
    " 'metric': 'auc',\n",
    " 'verbosity': -1,\n",
    " 'num_boost_round': 5575}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_clf = lgb.LGBMClassifier(**clf_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pkl_Filename = \"lgbm_model.pkl\"  \n",
    "\n",
    "with open(Pkl_Filename, 'wb') as file:  \n",
    "    pickle.dump(final_clf, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Roc Auc Score:\", roc_auc_score(y_test_split, final_clf.predict(X_test_split)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}